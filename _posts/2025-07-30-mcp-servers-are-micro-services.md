---
layout: post
title: MCP 서버로 만든 AI 에이전트를 프로덕션에 올려보기
excerpt: What to prepare for production with an MCP-powered LLM agent
author: haandol
email: ldg55d@gmail.com
tags: mcp msa observability llm efficiency agent
publish: true
---

## TL;DR

- MCP 환경에서 옵저버빌리티(로그·메트릭·트레이스) 없으면 디버깅 불가능한 블랙박스가 된다.
- 툴이 늘어날수록 토큰 비용·지연 시간이 선형이 아니라 계단식으로 증가한다.
- PoC에서 잘 돌던 챗봇이 프로덕션에서 망가지는 이유가 바로 이 두 가지.
- MSA 수준의 모니터링과 툴 다이어트·캐싱으로 토큰 예산을 관리해야 한다.
- 결국 MCP도 모놀리스→MSA 전환과 똑같이 조직·플랫폼·운영을 함께 성숙시켜야 한다.

## 시작하며

최근에 회사에서 MCP 기반으로 챗봇을 만들어보는 프로젝트를 하게 되었다.
PoC 단계에서는 정말 잘 돌았다. 데모 완성 속도가 빠르고, 관측할 필요도 없고, 토큰 비용도 미미했다.

- MCP 서버 2~3개 + 간단한 툴들 → 빠른 데모 완성 ✔  

그런데 막상 프로덕션에 올려보니... 지옥이었다.

- 10개 이상 툴 + 다중 LLM + 수천 TPS 환경
- 장애·성능 문제를 파악할 방법이 전혀 없음
- 월 청구서 폭증 + 모델이 툴 선택을 자주 실패
- 고객 지원 티켓이 폭주하기 시작

이런 경험을 토대로 "PoC에서 잘 돌았는데 운영해보니 지옥이더라"를 방지하기 위한 체크리스트를 정리해본다.

## 1. MCP 환경에 MSA급 옵저버빌리티 구축하기

개인적으로 이게 가장 중요하다고 생각한다.

MCP 환경은 여러 서버가 분산되어 동작하는 구조라 MSA와 매우 비슷하다.

따라서 MSA 수준의 운영경험이 없다면 기존 툴 방식과의 장단점을 더욱 신중하게 고려해서 MCP 서버 방식을 도입해야 한다.

| 단계 | 해야 할 일 | 내가 쓰고 있는 스택 |
|------|-----------|---------------|
| **① 로그** | 구조화된 JSON 로그, `trace_id` 반드시 포함 | Fluent Bit → Loki / CloudWatch Logs |
| **② 메트릭** | 툴 호출 TPS·지연시간·오류율·토큰 사용량 | Prometheus + Grafana |
| **③ 트레이싱** | OpenTelemetry로 LLM → Agent → 툴 전구간 추적 | Jaeger, 1% 샘플링부터 시작 |
| **④ 서비스 맵** | 의존성 자동 시각화 | Jaeger Service Dependencies |
| **⑤ 알람** | 95-퍼센타일 지연 > X ms, 오류율 > Y% | PagerDuty → Slack |
| **⑥ 헬스체크** | 각 툴마다 `/healthz` 엔드포인트 | 서비스 메시 Liveness probe |
| **⑦ 환경분리** | 프로덕션 토큰 등 민감정보 스테이징 격리 | Terraform으로 환경 분리 |

**실무에서 도움된 팁들**
- OpenTelemetry 트레이스를 통해 툴 호출 전구간을 추적할 수 있다. (대부분의 LLM 추적 도구들은 OpenTelemetry 를 기본으로 지원하는 추세이다.)
- Trace ID 연동하면 로그 ↔ 트레이스 UI 이동이 한 클릭으로 가능하다.
- Token Usage를 `input_tokens`, `output_tokens` 별도 그래프로 만들어두면 비용 최적화할 때 유용하다.
- 실패한 Trace payload 저장해서 재현 테스트에 활용할 수 있다.

개인적으로는 Arize Phoenix 를 옵저버빌리티에 사용하고 있다.

## 2. MCP 와 툴은 컨텍스트

에이전트에 툴을 쓰든 MCP 서버를 붙이든 결국 LLM 이 호출된다는 사실에는 변함이 없다.

그리고 툴이나 MCP 가 API 상에 필드를 별도로 입력받는다고 해서 LLM 이 뭔가 추가적인 기능을 갖는 것은 아니다.

LLM 은 결국 텍스트를 입력받고 텍스트를 출력하는 기계이다. 해당 입출력 텍스트를 우리는 컨텍스트라고 부른다.

따라서 내가 클라이언트에서 어떤 방식으로 툴을 입력을 하든(하드코딩된 프롬프트, MCP, API 의 JSON 필드), 결국 컨텍스트 형태로 LLM 에게 전달되어 실행되기 때문에 컨텍스트 관점에서 툴을 봐야 한다.


## 3. 툴이 늘어나면 생기는 문제들

우리는 LLM 의 입력 컨텍스트가 커질수록 정확도가 떨어지고 레이턴시가 증가하며, 비용이 커진다는 것을 알고 있다.

아래는 이를 툴 관점에서 실험한 결과이다. (GPT-4o-32k, 함수 호출 모드, 동일 질의 1,000회 평균)

| 툴 개수 | 툴 설명 토큰 | 1회 호출시 추가 토큰 | 응답 지연 | 툴 선택 정확도 |
|-------|-------------|------------------|----------|-------------|
| 2개   | 120개        | +240개           | +80ms    | 98% |
| 6개   | 110개        | +660개           | +320ms   | 92% |
| 15개  | 105개        | +1,575개         | +900ms   | 78% |
| 30개  | 95개         | +2,850개         | +1,800ms | 63% |

보다시피 툴이 늘어날수록 선형이 아니라 **계단식으로** 비용·지연이 상승한다. 모델도 유사한 툴들 간에 혼동을 많이 한다.

특히 여러 MCP 서버가 서로 다른 조직에서 작성되고 관리되는 경우, 툴의 설명을 통일감있게 작성하는 것이 매우 중요한데, 툴 설명에 사용되는 단어가 서로 다른 의미로 사용하거나, 설명이 겹치거나, 모호하게 작성된 경우 정확도가 더욱 내려간다. 

LLM 프로바이더에 따라 최대 128개 툴을 등록할 수는 있지만, **실제로는 10개 이상부터 성능 저하가 발생**한다.

MCP 서버 하나당 보통 3~4개 툴을 제공하니까, **최대 3개 MCP 서버 정도가 적당**하다.

### 툴 다이어트 전략

- **툴은 생각보다 비싸다.** 적은 툴일수록 레이턴시↓, 토큰 소모↓, 정확도↑
- **툴은 API가 아니다.** 더 적은 툴로 더 많은 작업을 할 수 있게 설계하자.
- **탑다운 방식**으로 툴 경계를 잡는 게 바텀업보다 훨씬 좋다.
- 파라미터가 너무 커지면 차라리 **코드로 직접 쿼리**하는 게 나을 수도 있다. (python repl, sql, graphql 등)

### 토큰·비용 폭증 완화 전략

1. **동적 툴 필터링**
   - 라우터 모델이나 룰 기반으로 현재 질의에 필요한 툴만 프롬프트에 포함
2. **툴 메타데이터 다이어트**  
   - 설명·파라미터 문구 축약, 예시 JSON 최소화
3. **대화 컨덴싱**  
   - 오래된 대화 → 요약문으로 교체, Retrieval만 사용
4. **캐싱 적극 활용**  
   - Semantic/프롬프트 툴 캐시로 동일 질의·툴 결과 재사용
5. **가드레일 설정**  
   - `max_tool_calls` 설정으로 무한 루프 방지
   - 검색 툴, 결제 툴 등은 전용 에이전트로 분리해서 필요시에만 마운트

## 3. PoC → 프로덕션 체크리스트

- [ ] **옵저버빌리티 스택 완성** (로그·메트릭·트레이스 대시보드 & 알람)  
- [ ] **툴 필터 라우터** 적용, 프롬프트 토큰 < N k 제한  
- [ ] **각 툴 TPS 제한** 레이트 리밋·서킷 브레이커 설정  
- [ ] **월간 토큰 예산 알람** (비용 5% 초과시 Slack 경보)  
- [ ] **롤백 플랜** (새 툴 배포 실패시 자동 언마운트)  
- [ ] **보안 게이트** (툴 등록 PR → LLM 세이프가드 체크 후 병합)  
- [ ] **부하 테스트** (예상 피크 TPS × 2배 트래픽)  
- [ ] **DR/HA 전략** (MCP 서버 및 툴 컨테이너 멀티 AZ 배치, RTO ≤ 15분)  

## 마치며

MCP는 LLM의 USB 포트 라는 말 처럼, 다양한 툴을 연결할 수 있다는 점에서 매력적이다. 하지만 진짜 PC 처럼 포트수에 한계가 있다.

포트 수는 결국 LLM 이 처리할 수 있는 유효 컨텍스트 크기이며, 컨텍스트 증가에 따른 정확도 문제가 같이 개선되지 않으면, 컨텍스트 크기가 커지고 가격만 내려간다고 해서 포트 수가 늘어나지는 않을 것이다.

따라서 툴 선택과 프롬프트 다이어트, 캐싱 등 토큰 예산을 관리하는 것이 중요하다.

**가장 많이 실패하는 패턴**
1. 툴을 왕창 등록하고
2. 관측·제어 없이 운영 투입
3. 비용·오류 폭발

개인적으로는 PoC와 프로덕션 사이의 'Chasm'을 건너려면 **모이고(툴), 보이고(Observability), 줄이고(Token)** 세 단어가 핵심이라고 생각한다.

- **모이되** 도메인별로 쪼개서 관리하고
- **보이게** 만들어서 문제를 즉시 감지·해결하며  
- **줄여서** 비용과 혼돈을 통제할 수 있을 때