---
layout: post
title: MCP 서버 기반 에이전트 프로덕션 준비하기
excerpt: What to prepare for production with an MCP-powered LLM agent
author: haandol
email: ldg55d@gmail.com
tags: mcp msa observability llm efficiency agent
publish: true
---

## TL;DR

- **분산 옵저버빌리티**(로그·메트릭·트레이스)를 갖추지 않은 MCP 환경은 디버깅이 불가능한 블랙박스가 된다.  
- 툴이 늘어날수록 **컨텍스트 토큰 소모·지연 시간·비용**이 선형이 아니라 **계단식**으로 증가한다.  
- PoC에서는 잘 돌아가던 챗봇이 **프로덕션에서 오작동·반응 지연·요금 폭탄**으로 이어지는 이유가 바로 이 두 축.  
- 문제를 예방하려면 **(1) MSA 수준의 모니터링 스택 구축, (2) 툴 선택·프롬프트 다이어트·캐싱**을 통해 토큰 예산을 관리해야 한다.  
- 결국 MCP 도입은 모놀리스에서 MSA로 넘어갈 때와 똑같이 **조직, 플랫폼, 오퍼레이션**을 함께 성숙시켜야 한다.

## 시작하며

- PoC 단계
  - MCP 서버 + 2~3개 툴 → 빠른 데모 ✔  
  - 관측 포인트 없음, 토큰 비용 무시해도 미미  
- 프로덕션 단계
  - 10 개 이상 툴 + 다중 LLM + 수천 TPS  
  - 장애·성능 문제를 눈으로 볼 방법이 없고, 월 청구서가 폭증  
  - 모델이 툴 선택에 자주 실패 → 고객 지원 티켓 폭주  

이 글은 PoC에서 잘 돌았는데 운영해 보니 지옥이더라… 를 막기 위한 체크리스트다.

## 1. MCP 도입 시 MSA급 분산 옵저버빌리티 구축 작업

| 단계 | 해야 할 일 | 추천 스택 & 팁 |
|------|-----------|---------------|
| **① 로그** | 구조화 JSON 로그로 통일, `trace_id` 포함 | Fluent Bit → Loki / CloudWatch Logs |
| **② 메트릭** | 툴 호출 TPS·지연 시간·오류율·토큰 사용량 노출 | Prometheus + Grafana 대시보드 |
| **③ 트레이싱** | OpenTelemetry SDK로 LLM → Agent → 툴 전 구간 스팬 생성 | Jaeger / Zipkin, 1 % 샘플링부터 시작 |
| **④ 서비스 맵** | 트레이스 데이터를 자동 집계 → 의존성 시각화 | Jaeger Service Dependencies |
| **⑤ 경보 & SLO** | 95-퍼센타일 지연 > X ms, 오류율 > Y % 알람 | PagerDuty / SNS → Slack 알림 |
| **⑥ 헬스체크** | 각 툴 `/healthz` 엔드포인트, 슬로우 스타트 배포 | 서비스 메시(L4/7 프록시) Liveness probe |
| **⑦ 샌드박스** | 프로덕션 Data Dog token 등 민감 설정 → 스테이지 격리 | IaC (Terraform / CDK)로 환경 구분 |

**현업 팁**  
- Trace ID 연동: 로그 ↔ 트레이스 UI 이동이 한 클릭.  
- Token Usage Metric: `input_tokens`, `output_tokens` 별도 그래프화.  
- 리플레이 환경: 실패한 Trace payload 저장 → 재현 테스트에 사용.

## 2. 툴 증가 = 컨텍스트 폭증

| 툴 수 | 툴 설명 토큰(평균) | 1 회 호출 시 추가 토큰 | 응답 지연(Δ) | 툴 선택 정확도 |
|-------|------------------|------------------------|--------------|----------------|
| 2     | 120              | +240                   | +80 ms       | 98% |
| 6     | 110              | +660                   | +320 ms      | 92% |
| 15    | 105              | +1 575                 | +900 ms      | 78% |
| 30    | 95               | +2 850                 | +1 800 ms    | 63% |

테스트 환경: GPT-4o-32k, 함수 호출 모드, 동일 질의 1 000 회 평균.  

결론: 툴 설명이 길어질수록 선형보다 가파른 비용·지연 상승 → 모델은 유사 툴 간 혼동 ↑.

### 토큰·비용 폭증 완화 전략

1. 동적 툴 필터링  
   - 라우터 모델 or 룰 기반으로 현재 질의에 필요한 툴만 프롬프트에 삽입.  
2. 툴 메타데이터 다이어트  
   - 설명·파라미터 문구 축약, 예시 JSON 최소화.  
3. 대화 컨덴싱  
   - 오래된 턴 → 요약문으로 교체, Retrieval 만 사용.  
4. Semantic / HTTP 캐시  
   - 동일 질의·툴 결과 재사용 → LLM 재호출 억제.  
5. 호출 횟수 가드레일  
   - `max_tool_calls` 설정, 무한 루프
   - 검색 툴, 결제 툴 등 전용 에이전트로 쪼개고 필요 시점에만 마운트.  

## 3. PoC → 프로덕션 체크리스트

- [ ] Observability Stack 배포 완료 (로그·메트릭·트레이스 대시보드 & 알람)  
- [ ] Tool Filter Router 적용, 프롬프트 토큰 < N k 제한  
- [ ] 각 툴 TPS 레이트 리밋·서킷 브레이커 설정  
- [ ] 월간 Token Budget 알람 (예: 비용 5 % 초과 시 Slack 경보)  
- [ ] Rollback 플랜 (새 툴 배포 실패 시 자동 언마운트)  
- [ ] 보안 게이트 (툴 등록 PR → LLM 세이프가드 체크 통과 후 병합)  
- [ ] 부하 테스트 (예상 피크 TPS × 2 배 traffic with synthetic queries)  
- [ ] DR/HA 전략 (MCP 서버 및 툴 컨테이너 멀티 AZ 배치, RTO ≤ 15 분)  

## 마치며

MCP는 LLM에 USB-C 포트를 단 것처럼 강력하다. 하지만 포트에 뭐든 꽂을 수 있다고 해서 엔터프라이즈급 신뢰성이 자동으로 생기지는 않는다.  

**가장 많이 실패하는 패턴**  
1. 툴을 왕창 등록하고,
2. 관측·제어 없이 운영에 투입,
3. 비용·오류 폭발.

PoC와 프로덕션 사이의 'Chasm'을 건너려면 모이고(툴), 보이고(Observability), 줄이고(Token) 세 단어를 기억하자.  
- **모이되** 도메인별로 쪼개어 관리하고,  
- **보이게** 만들어 문제를 즉시 감지·해결하며,  
- **줄여서** 비용과 혼돈을 통제할 수 있을 때, MCP 기반 LLM 에이전트는 진짜 ROI를 낳는다.

챗봇을 "보여주기용 데모"에 그치지 않게 하려면, **성능·비용·안정성** 세 축을 초기에 설계하자.